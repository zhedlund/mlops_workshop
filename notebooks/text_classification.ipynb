{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Your First ML Pipeline\n",
    "In this workshop we will work on a <strong>text classification</strong> task. The goal of text classification is to assign a label or a class to input text.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <<< *In case you are already familiar with the basics of text classification, you can skip the following intro and move on to __Dataset and model__ below it.* >>>\n",
    " \n",
    " &nbsp;\n",
    "\n",
    "\n",
    "It is a very common task in <strong>Natural Language Processing (NLP)</strong> field and is quite straightforward to start with!\n",
    "\n",
    "\n",
    "There are many uses cases for which text classification is used, some examples:\n",
    "\n",
    " \n",
    "* Sentiment analysis and toxicity detection (review score prediction, comment section analysis)\n",
    "\n",
    "* Topic classification (news article topic prediction, literature genre classification)\n",
    "\n",
    "\n",
    "* Intent detection (user intent classification for Chatbots, search query intent)\n",
    "\n",
    "\n",
    "* Spam detection, language identification etc.\n",
    "\n",
    "\n",
    "Depending on a task, the classification can be either:\n",
    "* Binary with only 2 classes\n",
    "\n",
    "* Multi-class - multiple mutually exclusive classes\n",
    "\n",
    "* Multi-label classification - not mutually exclusive, one entry can be assigned to none or to multiple labels\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <<< *End of skippable info section.* >>>\n",
    " \n",
    " &nbsp;\n",
    "\n",
    "## Dataset and model\n",
    "We will be working on an <strong>intent detection</strong> task. \n",
    "\n",
    "Our dataset is called  [<strong>Banking77</strong>](https://huggingface.co/datasets/PolyAI/banking77), it contains online banking queries annotated with their corresponding intents. All queries are in English.\n",
    "Overall, in this dataset there are 13,083 customer service queries labeled with 77 intents.\n",
    "\n",
    "<strong>Our goal:</strong> build a model which can predict the intent for a previously unseen user query. Each query needs to be assigned to a single intent which makes this problem a multi-class text classification.\n",
    "\n",
    "\n",
    "Predicting the intent of a user query is useful because it can make it possible to automate commonly needed functionality, save customers time and improve service satisfaction and ease load on customer support.\n",
    "\n",
    "# What we will do:\n",
    "* First, we will load the data and explore it\n",
    "\n",
    "\n",
    "* Then, we will convert our texts to their vector representation to prepare the data for model training \n",
    "\n",
    "\n",
    "* We will try out 2 simple models and select the best parameters to use for them\n",
    "\n",
    "\n",
    "* In the end, we will create a simple online API with Streamlit and test our solution live!\n",
    "\n",
    "## Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import what we will be using today. The needed libraries are:\n",
    "* [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) - very commonly used tabular data analysis and manipulation tool\n",
    "* [numpy](https://numpy.org/doc/stable/) -  also a very commonly used package for scientific computing in Python\n",
    "* [scikit-learn](https://scikit-learn.org/stable/) - another library which is commonly used in data science and contains lots of tools for machine learning and statistical modeling \n",
    "* [sentence_transformers](https://sbert.net/) -  a library for accessing, using, and training embedding models, part of the Hugging Face ecosystem. We will use it to access one of the embeddings models to vectorize our texts.\n",
    "* [datasets](https://huggingface.co/docs/datasets/en/index) - a library for easily accessing and sharing datasets, part of the Hugging Face ecosystem. We will only be using it to access the Banking77 dataset.\n",
    "* [mlflow](https://mlflow.org/docs/latest/index.html) - an open-source platform for machine learning developments, most often used for model, artifact, parameter logging and sharing, experiment (model runs) comparison. \n",
    "\n",
    "They are already being defined and installed in the requirements.txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the [dataset](https://huggingface.co/datasets/PolyAI/banking77):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"PolyAI/banking77\", revision=\"main\") # taking the data from the main branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is split in training and testing sets. Both have 2 columns: text and label. Training data contains 10,003 rows and test data contains 3,080 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about how to work with the datasets library [here](https://huggingface.co/docs/datasets/en/quickstart), but we won't need anything other than the function above for this workshop.\n",
    "\n",
    "Let's convert the datasets to pandas library DataFrame format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(dataset['train'])\n",
    "test_data = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas makes working with data and doing simple data analysis very convenient. Below we will use some of the pandas functions like [head](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html), [value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html), [describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html), [plot](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) for analysis.\n",
    "\n",
    "First, let's look at the top rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are numerical as you can see. These are positional indexes from the list of intent names (below) for this dataset. Let's add the labels names to make the data more interpretable. This will be useful since these labels are the intent which we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"activate_my_card\",\n",
    "    \"age_limit\",\n",
    "    \"apple_pay_or_google_pay\",\n",
    "    \"atm_support\",\n",
    "    \"automatic_top_up\",\n",
    "    \"balance_not_updated_after_bank_transfer\",\n",
    "    \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    \"beneficiary_not_allowed\",\n",
    "    \"cancel_transfer\",\n",
    "    \"card_about_to_expire\",\n",
    "    \"card_acceptance\",\n",
    "    \"card_arrival\",\n",
    "    \"card_delivery_estimate\",\n",
    "    \"card_linking\",\n",
    "    \"card_not_working\",\n",
    "    \"card_payment_fee_charged\",\n",
    "    \"card_payment_not_recognised\",\n",
    "    \"card_payment_wrong_exchange_rate\",\n",
    "    \"card_swallowed\",\n",
    "    \"cash_withdrawal_charge\",\n",
    "    \"cash_withdrawal_not_recognised\",\n",
    "    \"change_pin\",\n",
    "    \"compromised_card\",\n",
    "    \"contactless_not_working\",\n",
    "    \"country_support\",\n",
    "    \"declined_card_payment\",\n",
    "    \"declined_cash_withdrawal\",\n",
    "    \"declined_transfer\",\n",
    "    \"direct_debit_payment_not_recognised\",\n",
    "    \"disposable_card_limits\",\n",
    "    \"edit_personal_details\",\n",
    "    \"exchange_charge\",\n",
    "    \"exchange_rate\",\n",
    "    \"exchange_via_app\",\n",
    "    \"extra_charge_on_statement\",\n",
    "    \"failed_transfer\",\n",
    "    \"fiat_currency_support\",\n",
    "    \"get_disposable_virtual_card\",\n",
    "    \"get_physical_card\",\n",
    "    \"getting_spare_card\",\n",
    "    \"getting_virtual_card\",\n",
    "    \"lost_or_stolen_card\",\n",
    "    \"lost_or_stolen_phone\",\n",
    "    \"order_physical_card\",\n",
    "    \"passcode_forgotten\",\n",
    "    \"pending_card_payment\",\n",
    "    \"pending_cash_withdrawal\",\n",
    "    \"pending_top_up\",\n",
    "    \"pending_transfer\",\n",
    "    \"pin_blocked\",\n",
    "    \"receiving_money\",\n",
    "    \"Refund_not_showing_up\",\n",
    "    \"request_refund\",\n",
    "    \"reverted_card_payment?\",\n",
    "    \"supported_cards_and_currencies\",\n",
    "    \"terminate_account\",\n",
    "    \"top_up_by_bank_transfer_charge\",\n",
    "    \"top_up_by_card_charge\",\n",
    "    \"top_up_by_cash_or_cheque\",\n",
    "    \"top_up_failed\",\n",
    "    \"top_up_limits\",\n",
    "    \"top_up_reverted\",\n",
    "    \"topping_up_by_card\",\n",
    "    \"transaction_charged_twice\",\n",
    "    \"transfer_fee_charged\",\n",
    "    \"transfer_into_account\",\n",
    "    \"transfer_not_received_by_recipient\",\n",
    "    \"transfer_timing\",\n",
    "    \"unable_to_verify_identity\",\n",
    "    \"verify_my_identity\",\n",
    "    \"verify_source_of_funds\",\n",
    "    \"verify_top_up\",\n",
    "    \"virtual_card_not_working\",\n",
    "    \"visa_or_mastercard\",\n",
    "    \"why_verify_identity\",\n",
    "    \"wrong_amount_of_cash_received\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign label names from the list based on the index of the label \n",
    "train_data[\"label_name\"] = train_data[\"label\"].apply(lambda x: label_names[x])\n",
    "test_data[\"label_name\"] = test_data[\"label\"].apply(lambda x: label_names[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top rows again. Now we have the label name as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape (# of row and columns of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "f\"\"\"\n",
    "      Train dataset shape: {train_data.shape}, \n",
    "      Test dataset shape: {test_data.shape}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a random sample of 20 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To check the sample counts for the labels we have. We can use the pandas [value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) function for this.\n",
    "\n",
    "\n",
    "Looks like intent with the most labeled examples in the training data has 187 samples and is called \"card_payment_fee_charged\". The intent with the least amount of examples has 35 samples and is called \"contactless_not_working\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"label_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the statistics on the count of labeled examples per intent with the [describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function below.\n",
    "\n",
    "\n",
    "* We have 77 unique intents\n",
    "* Mean number of examples per intent is 129, with standard deviation of 32\n",
    "* The minimum is 35 examples and the maximum is 187\n",
    "* The 25th percentile is 112 examples, median is 127, 75th percentile is 159\n",
    "\n",
    "\n",
    "Seems like we have some deviations with less labeled intents, but overall the labels are relatively balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"label_name\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the same distribution by using the *plot* function to create the bar plot of the sample count per label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"label_name\"].value_counts().plot.barh(title='Label Distribution', figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the lengths of the user query texts and the number of tokens (roughly words) per query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_length'] = train_data['text'].str.len()  # get the text length (https://pandas.pydata.org/docs/reference/api/pandas.Series.str.len.html)\n",
    "train_data['text_num_tokens'] = train_data['text'].str.split().str.len()  # split the text by spaces and get the length of the resulting list to calculate the number of tokens (https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how we can get the length of the text and the number of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Here is a text\"\n",
    "tokens = text.split()\n",
    "\n",
    "print(f\"Length of the text is: {len(text)}\")\n",
    "print(f\"Tokens of the text are: {tokens}\")\n",
    "print(f\"The number of tokens is: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [describe function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) from pandas to take a look at the statistics for user query text length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_length'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Calculate the statistics for the number of tokens in user queries using the pandas [describe function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html). In addition to the default values, include the 10th and 90th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the previous step, let's plot the histograms of the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_length'].plot.hist(title='Text Length', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_num_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_num_tokens'].plot.hist(title='Number of tokens', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a lot of these distributions by label name to see if there are any significant differences. We can use the [groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function of pandas to calculate the statistics with the *describe* function per label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length\n",
    "text_length_statistics = train_data.groupby('label_name')['text_length'].describe()\n",
    "text_length_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Now calculate these statistics for the number of tokens columns, also using groupby by label name. Find which label has the highest mean number of tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the mean values for user query text length and token count per label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length_statistics.sort_values(by='mean')['mean'].plot.barh(title='Mean Text Length', figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Create a graph for the number of tokens per label like is done above for text lengths, but pick a different statistic other than the mean. Make sure the graph is sorted from highest to lowest value as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "text_length_statistics.sort_values(by='mean')['mean'].plot.barh(title='Mean Text Length', figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are some differences between labels in terms of text length and token number. This can mean that they would be useful features for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <<< *In case you are already familiair with the concept of text embeddings, you can skip the following intro.* >>>\n",
    " \n",
    " &nbsp;\n",
    "\n",
    "To be able to train our model we need to convert our training data in the form of text into numerical representations. We also need to do the same for our test data to be able to make predictions. This conversion from text into numerical representations is also called vectorization.\n",
    "\n",
    "\n",
    "There are several ways to vectorize text data, one of them which is often used is text embeddings. Text embeddings are vectors (lists) or floating point numbers and they are designed to capture the semantic meaning and context of the words they represent. There are many models available which can be used for getting embeddings from given text. Some examples, from simple to closer to state-of-the-art, are:\n",
    "* [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* [FastText](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "* [Universal sentence encoder (USE)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder.ipynb)\n",
    "* [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert)\n",
    "* [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert)\n",
    "* [XLNet](https://huggingface.co/docs/transformers/en/model_doc/xlnet)\n",
    "* [MPNet](https://huggingface.co/docs/transformers/en/model_doc/mpnet)\n",
    "* [GPT](https://platform.openai.com/docs/guides/embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You can check more available models on, for example but not limited to, [sentence_transformers](https://sbert.net/docs/sentence_transformer/pretrained_models.html) and Hugging Face [model hub](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) (sentence similarity models linked), there are also others. \n",
    "\n",
    "\n",
    "Embeddings are useful in multiple kinds of tasks:\n",
    "* Text similarity measurement\n",
    "* Text classification\n",
    "* Semantic search\n",
    "* Text clusterization\n",
    "* Recommendations\n",
    "\n",
    "\n",
    "and others.\n",
    "\n",
    "\n",
    "Embeddings can also be limited to a single language or be multilingual and support a varying number of languages.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <<< *End of skippable intro on text embeddings* >>>\n",
    " \n",
    " &nbsp;\n",
    "\n",
    "For building our model, we will make use of the idea that very similar sentences likely belong to the same intent label.\n",
    "For our embeddings, we will take an MPNet English language model developed by Microsoft and called [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2). It was trained for sentence similarity tasks and therefore produces embeddings which work well for our needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how the embeddings can be accessed, we need to load the model with needed path and use the *encode* function. See [here](https://huggingface.co/sentence-transformers/all-mpnet-base-v2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "embeddings_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = embeddings_model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check on one of the queries from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model.encode('I am still waiting on my card?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the dimentions of the produced vector\n",
    "embeddings_model.encode('I am still waiting on my card?').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Semantic similarity between texts is often measured with cosine similarity. Mathematically, it's the cosine of the angle between 2 vectors, meaning the dot product of the vectors divided by the product of their lengths. When we want to calculate the cosine similarity of 2 texts, text embeddings can be used as the vector representation.\n",
    "\n",
    "\n",
    "Your task is to <strong>calculate the cosine similarity of 2 given texts</strong> using the all-mpnet-base-v2 embeddings model we downloaded above. You can implement the cosine similarity function yourself using numpy (or look up how to implement it in Python).\n",
    " \n",
    "Feel free to try out any of your own text examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_1 = \"When will my card be delivered?\"\t\n",
    "text_2 = \"How long will my card delivery take?\"\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done with task 4, let's proceed to training our model! But first we need to vectorize our train and test datasets using the embeddings model.\n",
    "\n",
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to train and test our model we need to convert all our user queries in the training and test datasets into embeddings.\n",
    "\n",
    "We can do so by putting all of the queries of respective datasets into lists with a *tolist* function and then applying the embedding model the same way as in the examples above. This will create 2 embedding matrices which are ready for model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_lists = train_data[\"text\"].tolist()\n",
    "test_text_lists = test_data[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command takes around 4-5 mins to run\n",
    "train_embeddings = embeddings_model.encode(train_text_lists, show_progress_bar=True)\n",
    "test_embeddings = embeddings_model.encode(test_text_lists, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <<< *In case you are already familiair with the basics of k-NN and/or Logistic Regression, you can skip the following two info sections and move on to the __K-nearest Neighbors__ section below them.* >>>\n",
    " \n",
    " &nbsp;\n",
    "\n",
    "We will train 2 simple models based on [K-nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) and [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithms.\n",
    "\n",
    "<strong>k-Nearest Neighbors (k-NN) Classification</strong>\n",
    "k-NN classification works by memorizing all the training examples and then classifying a new data point based on its proximity to these memorized examples. For prediction, the algorithm calculates the distance between the new data point and examples in the training set. It then identifies the k closest training examples, i.e. neighbors, and assigns to the new data point the class that is most common among these k neighbors. All neighbors can either count equally to the predicted class vote or the closer neighbors can have more weight. The final predictions are scores representing the class vote distribution.\n",
    "\n",
    "<strong>Logistic Regression</strong>\n",
    "Logistic regression operates by first computing a linear combination of the input features. The result is then passed through the logistic (sigmoid) function, which maps it to a probability between 0 and 1. This probability represents the likelihood of the data point belonging to the positive class. If the probability is above a certain threshold (typically 0.5 but this can be tuned depending on the needs) the data point is classified as belonging to the positive class. Otherwise, it is classified as belonging to the negative class. Logistic regression can also be extended to multi-class classification.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <<< *End of skippable info sections.* >>>\n",
    " \n",
    " &nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest Neighbors\n",
    "\n",
    "Let's start by fitting a K-nearest Neighbors model on the training data.\n",
    "We will use scikit-learn implementation: [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n",
    "\n",
    "Although there are a lot of details, the scikit-learn API is fairly simple to use in practice: each estimator has a `.fit(X, y)` method to train a model and a `.predict(X)` method to predict on unseen data.\n",
    "\n",
    "We will start by fitting the model with its default parameters and this will give us a baseline model, upon which we can improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KNeighborsClassifier and fit on training data\n",
    "model_baseline = KNeighborsClassifier()\n",
    "model_baseline.fit(train_embeddings, train_data['label_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the results on cross-validation (more on how cross-validation works in Model Evaluation section)\n",
    "cv_scores = cross_val_score(estimator=model_baseline, X=train_embeddings, y=train_data['label_name'], scoring=\"f1_macro\", cv=3)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "      Baseline model CV scores by fold: {cv_scores}, \n",
    "      Mean CV score {cv_scores.mean()}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tunning\n",
    "\n",
    "The process of finding the optimal hyperparameters for your task is called [hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html). It involves experimenting with different hyperparameter values of an algorithm to find the combination that yields the best model performance on a validation set. This can be done using techniques like [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), where all possible combinations are tried, or random search, where random combinations are tested.\n",
    "\n",
    "To tune hyperparameters for our models we will use [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). It optimizes hyperparameters by randomly sampling a specified number of combinations from a given range. Each combination is evaluated using [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html), and the best performing set is selected. This method is more efficient than exhaustive searches, as it explores the hyperparameter space more quickly while still finding effective parameter settings.\n",
    "\n",
    "### K-nearest Neighbors Hyperparameter tuning\n",
    "\n",
    "Below we define values for our hyperparameter choices, refer to the scikit-learn documentation for more details on [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) settings.\n",
    "\n",
    "We are using the F1-score metric since it is well suited for our task - customer support text classification. For this case, it's important to balance precision and recall which [F1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score) helps to achieve. \"Macro\" in F1-macro is an averaging strategy, it means that the metrics are calculated for each label separately, and then their unweighted mean is found for the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_neighbors\": [3, 5, 7, 9, 11, 13, 15], # number of neighbors\n",
    "    \"weights\": [\"distance\", \"uniform\"], # whether the votes from all neighbors should be counted equally or by distance to the prediction point\n",
    "    \"metric\": [\"cosine\", \"euclidean\" ], # which metric to use for distance calculation\n",
    "}\n",
    "\n",
    "# define RandomizedSearchCV\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=KNeighborsClassifier(n_jobs=-1),\n",
    "    param_distributions=params,\n",
    "    n_iter=20,\n",
    "    random_state=0,\n",
    "    cv=3,\n",
    "    scoring=\"f1_macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter tunining given the train data (train_embeddings) and target values (train_data['label_name']), command takes around 40s to run\n",
    "search = clf.fit(train_embeddings, train_data['label_name']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the best hyperparameters found and the achieved F1-score on cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "      Best parameters: {search.best_params_}\n",
    "      F1-score: {search.best_score_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the cross-validation score by fold for the model with tuned hyperparameters:\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "cv_scores_tuned = cross_val_score(estimator=best_model, X=train_embeddings, y=train_data['label_name'], scoring=\"f1_macro\", cv=3)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "      Best model CV scores by fold: {cv_scores_tuned}, \n",
    "      Mean CV scores: {cv_scores_tuned.mean()} # same as above for search.best_score_\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, although very slightly, the f1_score on cross validation has increased after tuning hyperparameters. Depending on the task and data this difference can be more dramatic so tuning hyperparameters is worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Tune hyperparameters for a Logistic regression model via RandomizedSearchCV like in the example above. Consider the following hyperparameters: `C`, `penalty` and `solver`, but you can also add any other ones you want to tune! Refer to [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression) documentation for hyperparameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Hint:</strong> for the values of C you can define a disrtibution of values to check with [numpy](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) like this: `np.linspace(start=0.001, stop=10, num=21)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distributions = {\n",
    "    'C': , ### YOUR CODE HERE ### \n",
    "    'penalty': ### YOUR CODE HERE ### \n",
    "    'solver': ### YOUR CODE HERE ###\n",
    "}\n",
    "\n",
    "# Hint: use a higher number of iterations (n_iter=100) for RandomizedSearchCV since more combinations would be possible here than in k-NN example\n",
    "log_clf = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_search = log_clf.fit(train_embeddings, train_data['label_name']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "      Best parameters: {log_search.best_params_}\n",
    "      F1-score: {log_search.best_score_}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    " <<< *In case you are already familiair with the different components of model evaluation and cross-validation, feel free to skip the next info section.*>>>\n",
    " \n",
    " &nbsp;\n",
    " \n",
    "### Components of [Model Evaluation](https://scikit-learn.org/stable/model_selection.html):\n",
    "\n",
    "* [**Metric choice**](https://scikit-learn.org/stable/modules/model_evaluation.html): consider what is suitable for your task and model, as well as, what best reflects the requirements for the solution.\n",
    "  * For example, in a customer support chatbot, we might care about [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) equally and therefore F1 would be the optimal metric. Or it might be that precision is more important since we would rather transfer the request to a human when the bot is unsure. Then we could use a weighted F1 where precision is weighed higher.\n",
    "  * In some cases of binary classification, for example, it could be that we care about only the top N results with the highest prediction score, therefore we need to compute our metrics on this top set (it's called @k, for example, precision@k).\n",
    "  * Also, in some cases of binary classification, it's important that your true positive examples get a higher predicted score, then [ROC AUC score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) is a suitable metric. In other cases, it might not be important and then accuracy, F1, precision, and recall could work better.\n",
    "  * Don't use [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for imbalanced datasets since if your positive class is only 5% of the data, predicting all to be negative will give a 95% accuracy which seems high but is useless. Precision, recall, F1 would be better choices.  \n",
    "\n",
    "* **Validation set evaluation**: This is also a possibility, especially for non-standard cases when cross-validation is challenging to apply. Be careful not to overfit the validation set! Note that the validation set is different from a test set in this case. The data is split into 3 parts - training, validation, and testing.\n",
    "\n",
    "* **Test set evaluation**:\n",
    "  * The test set should not be used during data preprocessing, model training, or hyperparameter tuning to prevent data leakage and biasing the evaluation results.\n",
    "  * The purpose is to provide a final, unbiased evaluation of the model's performance.\n",
    "  * Be careful to make sure that the test set is representative of the real production data and that the way it was split is suitable for your task. The same considerations as for cross-validation exist here; time series data or not independently and identically distributed data need to be treated differently.\n",
    "  * Take note of the class imbalances in the test set compared to the production data. This can affect your metrics calculation reliability.\n",
    "  * Final evaluation is always best in the production setting (such as in an A/B test), since only that can show the real performance and impact of the model\n",
    "\n",
    "* [**Cross-validation**](https://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "  * Cross-validation works by splitting the training data into multiple subsets (folds). The model is trained on all of these subsets but one and is evaluated on this remaining one. This process is repeated several times (same as the number of folds), and the results are averaged.\n",
    "  * It is often used for hyperparameter tuning and model selection. By using multiple folds, it generally provides a more robust estimate of the model's performance compared to a single train-test split.\n",
    "  * Be careful with using cross-validation for time series data or data that is not independently and identically distributed (one row is not an independent entry, for example, the same user_id is present in the data multiple times). Use evaluation approaches suitable for your kind of task, such as [time series-aware validation splitting](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-of-time-series-data) or [Grouped cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#group-cv).\n",
    "\n",
    " &nbsp;\n",
    "\n",
    " <<< *End of skippable info section.*>>>\n",
    " \n",
    " &nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our k-NN model with the hyperparameters which we found in the previous step with RandomizedSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='cosine')\n",
    "knn.fit(train_embeddings,  train_data['label_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the predictions on the test set to measure the final metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_knn = knn.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our task of a customer support text classification, the metrics which are of interest to us are precision, recall and F1-score. We can use [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function to calculate them for all classes separately and also get the overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_data['label_name'], prediction_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report_dict = classification_report(test_data['label_name'], prediction_knn, output_dict=True) # get the same results in a dict format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report_dict['macro avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "1. Train a Logistic Regression model using the hyperparameters found in Task 5.\n",
    "\n",
    "2. Get predictions on the test data\n",
    "3. Calculate the metrics using classification_report function, as in the example above for k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your own query\n",
    "Once you are done with task 6, let's check our model on any new queries and output top 3 predicted classes with the highest scores. To access predicted scores we can use .predict_proba() instead of .predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_predictions(query, embed_model, clf_model, k):\n",
    "    # Function to output top k predicted classes with the highest scores\n",
    "    query_embedding = embed_model.encode(query).reshape(1, -1)\n",
    "    prediction_scores = filter(\n",
    "            lambda pr: pr[1] > 0,\n",
    "            zip(clf_model.classes_, clf_model.predict_proba(query_embedding)[0]),\n",
    "        ) # keep non 0 scores\n",
    "\n",
    "    prediction_result = sorted(prediction_scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    return prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"I still haven't recieved my card, when will it be ready?\" # feel free to change to your own query!\n",
    "get_top_k_predictions(query=test_query, embed_model=embeddings_model, clf_model=knn, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a more difficult one\n",
    "test_query = \"I don't see my transactions reflected in my account balance. How can I fix it?\"\n",
    "get_top_k_predictions(query=test_query, embed_model=embeddings_model, clf_model=knn, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Versioning with MLFLow\n",
    "\n",
    "Model versioning refers to save the model for later usage. \n",
    "Often is the case we want to separate the process of building the model from that of using the model for production.\n",
    "MLFLow is an open-source solution to do model versioning. Beyond saving the model, you can compare different versions of the model via an UI, add labels to the models (like production, testing, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///workspaces/build-your-first-ml-pipeline-workshop/mlruns\")\n",
    "mlflow.sklearn.log_model(knn, 'model')\n",
    "mlflow.log_metrics(clf_report_dict['macro avg'])\n",
    "mlflow.log_params(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7\n",
    "\n",
    "Load the saved model back to the notebook using its mlflow id.\n",
    "\n",
    "You can find the mlflow run id in `/notebooks/mlruns/0/<run_id>/..`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = ### YOUR CODE HERE ###\n",
    "model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\")\n",
    "model.predict(embeddings_model.encode(\"I still haven't recieved my card, when will it be ready?\").reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps: Creating the pipeline\n",
    "\n",
    "We finished the notebook work! Congratulations to reaching this point!\n",
    "\n",
    "Now go to: notebooks/test_pipeline.ipynb and fill in the missing lines in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Improvements tasks \n",
    "\n",
    "What you can do if you want to further improve your model:\n",
    "* [Tune the decision threshold to improve precision](https://scikit-learn.org/stable/modules/classification_threshold.html#)\n",
    "* Conduct error analysis - on a validation set (not test set!) check on which examples your model is making mistakes, any patterns you can draw from it which could be fixed by additional features, fixing labeling mistakes, tuning decision threshold?\n",
    "* Train a more complex model, to learn how to fine-tune a transformer model for text classification you can check these resources: Hugging Face [article](https://huggingface.co/docs/transformers/v4.17.0/en/tasks/sequence_classification) and [example notebook](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
